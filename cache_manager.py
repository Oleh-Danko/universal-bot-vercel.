import os
import json
import logging
from datetime import datetime
from typing import List, Dict
import requests
from bs4 import BeautifulSoup

logger = logging.getLogger("cache_manager")

CACHE_PATH = "cache/news_cache.json"
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0 Safari/537.36"
}

# === –î–∂–µ—Ä–µ–ª–∞ –¥–ª—è –∑–±–æ—Ä—É –Ω–æ–≤–∏–Ω ===
SOURCE_CONFIG = [
    ("Epravda ‚Äî Finances", "https://www.epravda.com.ua/finances"),
    ("Epravda ‚Äî Columns", "https://www.epravda.com.ua/columns"),
    ("Reuters ‚Äî Business", "https://www.reuters.com/business"),
    ("Reuters ‚Äî Markets", "https://www.reuters.com/markets"),
    ("Reuters ‚Äî Technology", "https://www.reuters.com/technology"),
    ("FT ‚Äî Companies", "https://www.ft.com/companies"),
    ("FT ‚Äî Technology", "https://www.ft.com/technology"),
    ("FT ‚Äî Markets", "https://www.ft.com/markets"),
    ("FT ‚Äî Opinion", "https://www.ft.com/opinion"),
    ("BBC ‚Äî Business", "https://www.bbc.com/business"),
]


def load_cache() -> Dict:
    """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∫–µ—à—É –∑ —Ñ–∞–π–ª—É."""
    if not os.path.exists(CACHE_PATH):
        return {"timestamp": None, "articles": []}
    try:
        with open(CACHE_PATH, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ —á–∏—Ç–∞–Ω–Ω—è –∫–µ—à—É: {e}")
        return {"timestamp": None, "articles": []}


def save_cache(cache: Dict):
    """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–µ—à—É."""
    os.makedirs(os.path.dirname(CACHE_PATH), exist_ok=True)
    try:
        with open(CACHE_PATH, "w", encoding="utf-8") as f:
            json.dump(cache, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å—ñ –∫–µ—à—É: {e}")


def parse_source(url: str, source_label: str) -> List[Dict[str, str]]:
    """–ü–∞—Ä—Å–∏–Ω–≥ –æ–¥–Ω—ñ—î—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–æ–≤–∏–Ω."""
    news = []
    try:
        resp = requests.get(url, headers=HEADERS, timeout=10)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        # --- Epravda ---
        if "epravda.com.ua" in url:
            for a in soup.select("a.article__title, a.post__title, a.title, h3 a"):
                title = a.get_text(strip=True)
                href = a.get("href")
                if href and title:
                    link = requests.compat.urljoin(url, href)
                    news.append({"title": title, "link": link, "source": source_label})

        # --- Reuters ---
        elif "reuters.com" in url:
            for a in soup.select("a[href^='/']"):
                href = a.get("href")
                title = a.get_text(strip=True)
                if href and title:
                    if any(seg in href for seg in ["/business", "/markets", "/technology"]):
                        link = requests.compat.urljoin("https://www.reuters.com", href)
                        news.append({"title": title, "link": link, "source": source_label})

        # --- Financial Times ---
        elif "ft.com" in url:
            for a in soup.select("a.js-teaser-heading-link, a.o-teaser__heading[href]"):
                title = a.get_text(strip=True)
                href = a.get("href")
                if href and title:
                    link = requests.compat.urljoin("https://www.ft.com", href)
                    news.append({"title": title, "link": link, "source": source_label})

        # --- BBC Business ---
        elif "bbc.com/business" in url:
            for a in soup.select("a.gs-c-promo-heading[href]"):
                title = a.get_text(strip=True)
                href = a.get("href")
                if href and title:
                    link = requests.compat.urljoin("https://www.bbc.com", href)
                    news.append({"title": title, "link": link, "source": source_label})

    except Exception as e:
        logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {source_label}: {e}")

    return news


def run_cache_update() -> int:
    """–û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –∫–µ—à—É –∑ —É—Å—ñ—Ö –¥–∂–µ—Ä–µ–ª."""
    all_news = []
    seen = set()

    for source_label, url in SOURCE_CONFIG:
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ {source_label}...")
        items = parse_source(url, source_label)
        for item in items:
            if item["link"] not in seen:
                seen.add(item["link"])
                all_news.append(item)

    cache = {
        "timestamp": datetime.utcnow().isoformat(),
        "articles": all_news,
    }

    save_cache(cache)
    logger.info(f"‚úÖ –ö–µ—à –æ–Ω–æ–≤–ª–µ–Ω–æ ({len(all_news)} –Ω–æ–≤–∏–Ω).")
    return len(all_news)