import asyncio
import logging
import os
import cloudscraper
from bs4 import BeautifulSoup

# Playwright imports
from playwright.async_api import async_playwright

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –ª–æ–≥—É–≤–∞–Ω–Ω—è (Render –±–∞—á–∏—Ç—å print —É –ª–æ–≥–∞—Ö)
logging.basicConfig(level=logging.INFO)

async def fetch_bloomberg():
    """
    –ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π –ø–∞—Ä—Å–µ—Ä Bloomberg:
    1Ô∏è‚É£ –ü—Ä–æ–±—É—î cloudscraper.
    2Ô∏è‚É£ –Ø–∫—â–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –∑–∞–±–ª–æ–∫–æ–≤–∞–Ω–∞ –∞–±–æ –ø—É—Å—Ç–∞ ‚Äî fallback –¥–æ Playwright.
    3Ô∏è‚É£ –í–∏–≤–æ–¥–∏—Ç—å –ø–µ—Ä—à—ñ 500 —Å–∏–º–≤–æ–ª—ñ–≤ HTML —É –ª–æ–≥–∞—Ö –¥–ª—è –¥—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∏.
    4Ô∏è‚É£ –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤.
    """

    url = "https://www.bloomberg.com/"
    html = ""

    # --- –ö—Ä–æ–∫ 1: Cloudsraper ---
    try:
        scraper = cloudscraper.create_scraper()
        response = scraper.get(url, timeout=15)
        response.raise_for_status()

        html = response.text.strip()

        if len(html) < 1000 or "Please enable JavaScript" in html or "Cloudflare" in html:
            logging.warning("‚ö†Ô∏è HTML –≤–∏–≥–ª—è–¥–∞—î –ø—ñ–¥–æ–∑—Ä—ñ–ª–æ (Cloudflare/–∑–∞—Ö–∏—Å—Ç). –ü–µ—Ä–µ—Ö–æ–¥–∏–º–æ –Ω–∞ Playwright.")
            html = ""
        else:
            logging.info("‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ HTML —á–µ—Ä–µ–∑ cloudscraper.")
    except Exception as e:
        logging.warning(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—ñ cloudscraper: {e}")
        html = ""

    # --- –ö—Ä–æ–∫ 2: Playwright fallback ---
    if not html:
        logging.info("üîÅ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ Playwright –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Å—Ç–æ—Ä—ñ–Ω–∫–∏...")
        try:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context(user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/115.0.0.0 Safari/537.36"
                ))
                page = await context.new_page()
                await page.goto(url, wait_until="networkidle", timeout=30000)
                html = await page.content()
                await browser.close()
                logging.info("‚úÖ –û—Ç—Ä–∏–º–∞–Ω–æ HTML —á–µ—Ä–µ–∑ Playwright.")
        except Exception as e:
            logging.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ Playwright: {e}")
            html = ""

    # --- –ö—Ä–æ–∫ 3: –î—ñ–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –æ—Ç—Ä–∏–º–∞–Ω–æ–≥–æ HTML ---
    logging.info(f"üìÑ –ü–µ—Ä—à—ñ 500 —Å–∏–º–≤–æ–ª—ñ–≤ HTML:\n{html[:500]}\n--- END OF PREVIEW ---")

    # --- –ö—Ä–æ–∫ 4: –ü–∞—Ä—Å–∏–Ω–≥ ---
    if not html:
        logging.warning("‚ö†Ô∏è HTML –ø–æ—Ä–æ–∂–Ω—ñ–π ‚Äî –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ [].")
        return []

    soup = BeautifulSoup(html, "html.parser")
    titles = _extract_candidates_from_soup(soup)

    logging.info(f"üîπ –ó–Ω–∞–π–¥–µ–Ω–æ {len(titles)} –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ Bloomberg.")
    return titles[:10] if titles else []


def _extract_candidates_from_soup(soup: BeautifulSoup) -> list[str]:
    """–ü–æ—à—É–∫ —É—Å—ñ—Ö –º–æ–∂–ª–∏–≤–∏—Ö –≤–∞—Ä—ñ–∞–Ω—Ç—ñ–≤ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤."""
    candidates = set()

    # --- –í–∞—Ä—ñ–∞–Ω—Ç 1: data-component-type (–Ω–∞–π—á–∞—Å—Ç—ñ—à–∏–π) ---
    for tag in soup.find_all(attrs={"data-component-type": "Headline"}):
        text = tag.get_text(strip=True)
        if text:
            candidates.add(text)

    # --- –í–∞—Ä—ñ–∞–Ω—Ç 2: <a> –∑ aria-label –∞–±–æ role="heading" ---
    for a in soup.find_all("a", attrs={"role": "heading"}):
        text = a.get_text(strip=True)
        if text:
            candidates.add(text)
    for a in soup.find_all("a", attrs={"aria-label": True}):
        text = a.get_text(strip=True)
        if text:
            candidates.add(text)

    # --- –í–∞—Ä—ñ–∞–Ω—Ç 3: –∫–ª–∞—Å–æ–≤—ñ –ø–∞—Ç–µ—Ä–Ω–∏ ---
    class_patterns = ["headline", "story", "title", "article"]
    for c in class_patterns:
        for el in soup.find_all(class_=lambda v: v and c in v.lower()):
            text = el.get_text(strip=True)
            if text:
                candidates.add(text)

    # --- –í–∞—Ä—ñ–∞–Ω—Ç 4: –¢–µ–≥–∏ h1, h2 ---
    for h in soup.find_all(["h1", "h2"]):
        text = h.get_text(strip=True)
        if text:
            candidates.add(text)

    # --- –í–∞—Ä—ñ–∞–Ω—Ç 5: meta og:title ---
    for meta in soup.find_all("meta", attrs={"property": "og:title"}):
        text = meta.get("content")
        if text:
            candidates.add(text.strip())

    # --- –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è ---
    filtered = [
        t for t in candidates
        if len(t) > 5
        and not t.lower().startswith("bloomberg")
        and not "cookies" in t.lower()
        and not "javascript" in t.lower()
    ]

    return list(filtered)


# --- –¢–µ—Å—Ç–æ–≤–∏–π –∑–∞–ø—É—Å–∫ –ª–æ–∫–∞–ª—å–Ω–æ ---
if __name__ == "__main__":
    results = asyncio.run(fetch_bloomberg())
    print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏:", results)