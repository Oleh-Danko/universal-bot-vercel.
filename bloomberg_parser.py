"""
bloomberg_parser.py
–ê–¥–∞–ø—Ç–∏–≤–Ω–∏–π –ø–∞—Ä—Å–µ—Ä –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤ Bloomberg:
- Playwright (—Ä–µ–Ω–¥–µ—Ä DOM) -> –æ—Å–Ω–æ–≤–Ω–∏–π —à–ª—è—Ö
- cloudscraper -> fallback (—à–≤–∏–¥—à–µ, —è–∫—â–æ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –ø—Ä–æ—Å—Ç—ñ—à–∞)
–ü–æ–≤–µ—Ä—Ç–∞—î list[dict] —Ñ–æ—Ä–º–∞—Ç—É: [{"title": "...", "url": "..."}, ...]
"""

import os
import asyncio
import logging
from typing import List, Dict, Optional

from bs4 import BeautifulSoup

# optional imports (lazy)
try:
    import cloudscraper
except Exception:
    cloudscraper = None

try:
    from playwright.async_api import async_playwright
except Exception:
    async_playwright = None

LOG = logging.getLogger("bloomberg_parser")
logging.basicConfig(level=logging.INFO)

BLOOMBERG_URL = "https://www.bloomberg.com/"

# strings to ignore
IGNORE_TEXTS = {"More from this issue:", "More from this issue", "Read more", "Subscribe"}


async def _fetch_with_playwright(url: str, timeout: int = 60000) -> Optional[str]:
    """–†–µ–Ω–¥–µ—Ä —á–µ—Ä–µ–∑ Playwright ‚Äî –ø–æ–≤–µ—Ä—Ç–∞—î HTML –∞–±–æ None."""
    if not async_playwright:
        LOG.info("Playwright –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
        return None

    LOG.info("üîé Playwright: –Ω–∞–º–∞–≥–∞—î–º–æ—Å—å –æ—Ç—Ä–∏–º–∞—Ç–∏ —Ä–µ–Ω–¥–µ—Ä–æ–≤–∞–Ω–∏–π HTML...")
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True, args=["--no-sandbox"])
            context = await browser.new_context(
                user_agent=(
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
                ),
                locale="en-US",
            )
            page = await context.new_page()
            await page.goto(url, timeout=timeout)
            # –¥–æ—á–µ–∫–∞—î–º–æ—Å—å –º–µ—Ä–µ–∂–µ–≤–æ—ó –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ
            try:
                await page.wait_for_load_state("networkidle", timeout=timeout)
            except Exception:
                # —ñ–Ω–æ–¥—ñ networkidle –º–æ–∂–µ –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞—Ç–∏ ‚Äî –≤—Å–µ –æ–¥–Ω–æ –±–µ—Ä–µ–º–æ content
                LOG.debug("Playwright: networkidle —Ç–∞–π–º–∞—É—Ç, –ø—Ä–æ–¥–æ–≤–∂—É—î–º–æ")
            html = await page.content()
            await browser.close()
            LOG.info("‚úÖ Playwright: HTML –æ—Ç—Ä–∏–º–∞–Ω–æ")
            return html
    except Exception as e:
        LOG.error(f"Playwright error: {e}", exc_info=True)
        return None


def _fetch_with_cloudscraper(url: str, timeout: int = 15) -> Optional[str]:
    """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∏–π fetch —á–µ—Ä–µ–∑ cloudscraper —è–∫ –∫–µ—à–æ–≤–∞–Ω–∏–π/—à–≤–∏–¥–∫–∏–π –≤–∞—Ä—ñ–∞–Ω—Ç."""
    if not cloudscraper:
        LOG.info("cloudscraper –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ.")
        return None

    LOG.info("üîé cloudscraper: –Ω–∞–º–∞–≥–∞—î–º–æ—Å—å –æ—Ç—Ä–∏–º–∞—Ç–∏ HTML...")
    try:
        scraper = cloudscraper.create_scraper(browser={"browser": "chrome", "platform": "windows"})
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
            ),
            "Accept-Language": "en-US,en;q=0.9",
            "Referer": "https://www.google.com/",
        }
        resp = scraper.get(url, headers=headers, timeout=timeout)
        if resp.status_code == 200:
            LOG.info("‚úÖ cloudscraper: HTML –æ—Ç—Ä–∏–º–∞–Ω–æ")
            return resp.text
        else:
            LOG.warning(f"cloudscraper returned status {resp.status_code}")
            return None
    except Exception as e:
        LOG.error(f"cloudscraper error: {e}", exc_info=True)
        return None


def _extract_candidates_from_soup(soup: BeautifulSoup) -> List[Dict[str, str]]:
    """
    –ó–±–∏—Ä–∞—î –∫–∞–Ω–¥–∏–¥–∞—Ç–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∑ DOM:
    - article h1/h2/h3
    - a[href contains '/news/'] (—Ç–µ–∫—Å—Ç –ø–æ—Å–∏–ª–∞–Ω–Ω—è)
    - –∑–∞–≥–∞–ª—å–Ω—ñ h2/h3 –µ–ª–µ–º–µ–Ω—Ç–∏
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ø–∏—Å–æ–∫ dict: {"title": title, "url": url_or_empty}
    """
    candidates = []

    # 1) article –∑–∞–≥–æ–ª–æ–≤–∫–∏
    for article in soup.find_all("article"):
        # —à—É–∫–∞—î–º–æ h1/h2/h3 —É —Å—Ç–∞—Ç—Ç—ñ
        for tag_name in ("h1", "h2", "h3"):
            tag = article.find(tag_name)
            if tag:
                text = tag.get_text(strip=True)
                if text:
                    # –∑–Ω–∞–π–¥–µ–º–æ –ø–æ—Å–∏–ª–∞–Ω–Ω—è –≤ —Å–µ—Ä–µ–¥–∏–Ω—ñ article, —è–∫—â–æ —î
                    a = article.find("a", href=True)
                    url = a["href"] if a else ""
                    candidates.append({"title": text, "url": url})

    # 2) –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ /news/ (—á–∞—Å—Ç–æ –æ—Å–Ω–æ–≤–Ω—ñ —Å—Ç–∞—Ç—Ç—ñ –º–∞—é—Ç—å —Ç–∞–∫—ñ URL)
    for a in soup.find_all("a", href=True):
        href = a["href"]
        if "/news/" in href or "/article/" in href:
            text = a.get_text(strip=True)
            if text and len(text) > 10:
                url = href if href.startswith("http") else f"https://www.bloomberg.com{href}"
                candidates.append({"title": text, "url": url})

    # 3) –∑–∞–≥–∞–ª—å–Ω—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏ h2/h3 (—è–∫ fallback)
    for tag in soup.find_all(["h2", "h3"]):
        text = tag.get_text(strip=True)
        if text and len(text) > 10:
            # —è–∫—â–æ —É –∑–∞–≥–æ–ª–æ–≤–∫–∞ —î –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–µ –ø–æ—Å–∏–ª–∞–Ω–Ω—è ‚Äî –±–µ—Ä–µ–º–æ –π–æ–≥–æ
            url = ""
            parent_a = tag.find_parent("a", href=True)
            if parent_a:
                url = parent_a["href"]
                if not url.startswith("http"):
                    url = f"https://www.bloomberg.com{url}"
            candidates.append({"title": text, "url": url})

    return candidates


def _clean_and_dedupe(candidates: List[Dict[str, str]], max_n: int = 10) -> List[Dict[str, str]]:
    """–§—ñ–ª—å—Ç—Ä—É–≤–∞–Ω–Ω—è —Ç–∞ –¥–µ–¥—É–ø–ª—ñ–∫–∞—Ü—ñ—è –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤."""
    seen = set()
    out = []
    for c in candidates:
        t = c.get("title", "").strip()
        if not t or len(t) < 12:
            continue
        # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Å–ª—É–∂–±–æ–≤—ñ –ø—ñ–¥–∫–∞–∑–∫–∏
        if t in IGNORE_TEXTS or any(ign in t for ign in IGNORE_TEXTS):
            continue
        # –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        norm = " ".join(t.split()).lower()
        if norm in seen:
            continue
        seen.add(norm)
        url = c.get("url", "").strip()
        if url and not url.startswith("http"):
            url = f"https://www.bloomberg.com{url}"
        out.append({"title": t, "url": url})
        if len(out) >= max_n:
            break
    return out


async def fetch_bloomberg(top_n: int = 10) -> List[Dict[str, str]]:
    """
    –û—Å–Ω–æ–≤–Ω–∞ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è, —â–æ –ø–æ–≤–µ—Ä—Ç–∞—î –¥–æ top_n –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤.
    –°—Ç—Ä–∞—Ç–µ–≥—ñ—è:
      1) –°–ø—Ä–æ–±–∞ Playwright (—Ä–µ–Ω–¥–µ—Ä)
      2) –Ø–∫—â–æ –Ω–µ–º–∞—î Playwright –∞–±–æ –≤—ñ–Ω –ø—Ä–æ–≤–∞–ª–∏–≤—Å—è ‚Äî cloudscraper
      3) –ü–∞—Ä—Å–∏–Ω–≥ —á–µ—Ä–µ–∑ BeautifulSoup + –µ–≤—Ä–∏—Å—Ç–∏–∫–∏ —Å–µ–ª–µ–∫—Ç–æ—Ä—ñ–≤
    """
    html = None

    # 1. Playwright (–±—ñ–ª—å—à –Ω–∞–¥—ñ–π–Ω–∏–π)
    html = await _fetch_with_playwright(BLOOMBERG_URL) if async_playwright else None

    # 2. Fallback: cloudscraper
    if not html:
        html = _fetch_with_cloudscraper(BLOOMBERG_URL)

    if not html:
        LOG.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ HTML –Ω—ñ —á–µ—Ä–µ–∑ Playwright, –Ω—ñ —á–µ—Ä–µ–∑ cloudscraper")
        return []

    # –ü–∞—Ä—Å–∏–º–æ HTML
    soup = BeautifulSoup(html, "html.parser")
    candidates = _extract_candidates_from_soup(soup)
    results = _clean_and_dedupe(candidates, max_n=top_n)

    # –Ø–∫—â–æ –Ω–µ–º–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ ‚Äî –ø—Ä–æ–±—É—î–º–æ —ñ–Ω—à–∏–π –±–ª–æ–∫ (—à–∏—Ä—à–µ)
    if not results:
        LOG.info("Fallback: —à—É–∫–∞—î–º–æ –±—ñ–ª—å—à —à–∏—Ä–æ–∫—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏ (meta og:title —Ç–∞ title)...")
        meta_title = soup.find("meta", property="og:title")
        if meta_title and meta_title.get("content"):
            results.append({"title": meta_title["content"].strip(), "url": BLOOMBERG_URL})
        page_title = soup.title.string.strip() if soup.title else ""
        if page_title and page_title not in (r["title"] for r in results):
            results.append({"title": page_title, "url": BLOOMBERG_URL})

    LOG.info(f"–ó–Ω–∞–π–¥–µ–Ω–æ –∑–∞–≥–æ–ª–æ–≤–∫—ñ–≤: {len(results)}")
    return results[:top_n]


# –õ–æ–∫–∞–ª—å–Ω–∏–π –∑–∞–ø—É—Å–∫ –¥–ª—è —Ç–µ—Å—Ç—É
if __name__ == "__main__":
    import asyncio
    r = asyncio.run(fetch_bloomberg(10))
    for i, it in enumerate(r, 1):
        print(f"{i}. {it['title']} -> {it['url']}")