# live_parser.py
import asyncio
import re
from typing import List, Dict, Tuple
import aiohttp
from bs4 import BeautifulSoup

# --- –î–∂–µ—Ä–µ–ª–∞ (—Ä—ñ–≤–Ω–æ —Ç—ñ, —â–æ —Ç–∏ –≤–∏–º–∞–≥–∞–≤) ---
SOURCES = {
    "Epravda (Finances)": "https://epravda.com.ua/finances",
    "Epravda (Columns)": "https://epravda.com.ua/columns",
    "Reuters (Business)": "https://www.reuters.com/business",
    "Reuters (Markets)": "https://www.reuters.com/markets",
    "Reuters (Technology)": "https://www.reuters.com/technology",
    "FT (Companies)": "https://www.ft.com/companies",
    "FT (Technology)": "https://www.ft.com/technology",
    "FT (Markets)": "https://www.ft.com/markets",
    "FT (Opinion)": "https://www.ft.com/opinion",
    "BBC (Business)": "https://www.bbc.com/business",
}

# --- –ó–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –æ–±–º–∞–Ω—É –∞–Ω—Ç–∏–±–æ—Ç—ñ–≤ —ñ 401 —É Reuters/FT/BBC ---
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/129.0.0.0 Safari/537.36"
    ),
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.9,uk;q=0.8",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
}

# --- –§—ñ–ª—å—Ç—Ä–∏ —Å–º—ñ—Ç—Ç—è —É —à–ª—è—Ö–∞—Ö ---
BAD_PATH_PARTS = {
    "privacy", "cookies", "terms", "about", "contact", "help", "faq", "signup",
    "subscribe", "newsletters", "account", "advert", "advertising", "policy",
    "store", "shop", "sitemap", "index", "live", "video", "watch-live", "audio",
    "bbcverify", "weather", "sport", "travel", "culture", "reel", "innovation",
    "usingthebbc", "pages", "mediakit", "projects", "press-release", "aboutthebbc"
}
BAD_EXTS = (".jpg", ".jpeg", ".png", ".gif", ".svg", ".ico", ".css", ".js", ".mp3", ".mp4", ".mov", ".m3u8")

# --- –ö–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ —É –Ω–æ–≤–∏–Ω–Ω–∏—Ö URL, —â–æ–± –ª–∏—à–∏–ª–æ—Å—å —Å–∞–º–µ ¬´news¬ª ---
GOOD_URL_HINTS = {
    "/news", "/article", "/story", "/stories", "/business", "/markets", "/technology",
    "/companies", "/opinion", "/economy", "/finance", "/finances", "/biznes", "/svit",
    "/power", "/publications", "/tehnologiji"
}

TITLE_MIN_LEN = 15    # —â–æ–± –≤—ñ–¥—Å—ñ—è—Ç–∏ ¬´Home¬ª, ¬´Europe¬ª, ¬´Login¬ª, ¬´About¬ª —Ç–æ—â–æ
TITLE_MAX_LEN = 220   # –ø—Ä–æ—Å—Ç–æ –±–µ–∑–ø–µ–∫–∞, —â–æ–± –Ω–µ –±—Ä–∞—Ç–∏ –Ω–∞–¥—Ç–æ –¥–æ–≤–≥—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏
MAX_PER_SOURCE = 120  # –º–∞–∫—Å–∏–º—É–º –Ω–∞ –¥–∂–µ—Ä–µ–ª–æ (—â–æ–± –Ω–µ –∑–∞–≤–∞–ª—é–≤–∞—Ç–∏ —á–∞—Ç)


def _is_bad_url(url: str) -> bool:
    if not url or url.startswith("#") or url.startswith("javascript:"):
        return True
    if any(url.endswith(ext) for ext in BAD_EXTS):
        return True
    low = url.lower()
    if any(part in low for part in BAD_PATH_PARTS):
        return True
    return False


def _looks_like_news_url(url: str) -> bool:
    low = url.lower()
    return any(hint in low for hint in GOOD_URL_HINTS)


def _clean_text(s: str) -> str:
    s = re.sub(r"\s+", " ", s or "").strip()
    # –ø—Ä–∏–±–∏—Ä–∞—î–º–æ –∑–∞–π–≤—ñ –∫—Ä–∞–ø–∫–∏/–¥–µ—Ñ—ñ—Å–∏ –≤ –∫—ñ–Ω—Ü—ñ
    s = re.sub(r"[‚Äì‚Äî\-‚Ä¢\¬∑\|:\s]+$", "", s)
    return s


def _pick_description(a_tag, container) -> str:
    """–ü—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ –æ–ø–∏—Å –ø–æ—Ä—É—á: –Ω–∞–π–±–ª–∏–∂—á–∏–π <p>, –∞–±–æ summary-—Å–ø–∞–Ω/–¥–∏–≤."""
    # 1) –Ω–∞–π–±–ª–∏–∂—á–∏–π –Ω–∞—Å—Ç—É–ø–Ω–∏–π <p>
    if container:
        p = container.find("p")
        if p:
            desc = _clean_text(p.get_text(" ", strip=True))
            if len(desc) >= 20:
                return desc
        # —ñ–Ω–æ–¥—ñ summary —É <div> –∞–±–æ <span>
        for cand in container.find_all(["div", "span"], limit=4):
            text = _clean_text(cand.get_text(" ", strip=True))
            if len(text) >= 30 and len(text) <= 260:
                return text

    # 2) –ø–æ–¥–∏–≤–∏–º–æ—Å—å –±–µ–∑ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ ‚Äî —É –±–∞—Ç—å–∫—ñ–≤—Å—å–∫–∏—Ö –µ–ª–µ–º–µ–Ω—Ç–∞—Ö –¥–æ 2 —Ä—ñ–≤–Ω—ñ–≤
    parent = a_tag.parent
    depth = 0
    while parent and depth < 2:
        p = parent.find("p")
        if p:
            desc = _clean_text(p.get_text(" ", strip=True))
            if len(desc) >= 20:
                return desc
        parent = parent.parent
        depth += 1
    return ""


def _collect_by_selectors(soup: BeautifulSoup, selectors: List[Tuple[str, str]]) -> List[Dict]:
    """–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π –∑–±—ñ—Ä –∑–∞ CSS-—Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º–∏: [(selector, source_name), ...] ‚Äî –∞–ª–µ —Ç—É—Ç source_name –Ω–µ –ø–æ—Ç—Ä—ñ–±–µ–Ω."""
    items = []
    seen = set()

    for sel, _ in selectors:
        for node in soup.select(sel):
            # —à—É–∫–∞—î–º–æ <a>
            a = node if node.name == "a" else node.find("a", href=True)
            if not a or not a.get("href"):
                continue

            url = a["href"]
            title = _clean_text(a.get_text(" ", strip=True))

            if not title or len(title) < TITLE_MIN_LEN or len(title) > TITLE_MAX_LEN:
                continue
            if _is_bad_url(url):
                continue

            # –∞–±—Å–æ–ª—é—Ç–∏—Ç–∏ —á–∞—Å—Ç–∫–æ–≤—ñ URL
            # (–∑–∞–ª–∏—à–∏–º–æ —è–∫ —î ‚Äî —É –Ω–∞—à–∏—Ö –¥–∂–µ—Ä–µ–ª –∑–∞–∑–≤–∏—á–∞–π –∞–±—Å–æ–ª—é—Ç–Ω—ñ –ø–æ—Å–∏–ª–∞–Ω–Ω—è; —è–∫—â–æ –Ω—ñ, –±—Ä–∞—É–∑–µ—Ä –≤—Å–µ –æ–¥–Ω–æ –∑—Ä–æ–∑—É–º—ñ—î)
            if not _looks_like_news_url(url):
                # —è–∫—â–æ —Å–µ–ª–µ–∫—Ç–æ—Ä —É–∂–µ "—Ç–æ—á–Ω–∏–π" (–≤—Å–µ—Ä–µ–¥–∏–Ω—ñ —Å—Ç–∞—Ç–µ–π), –º–æ–∂–Ω–∞ –¥–æ–∑–≤–æ–ª–∏—Ç–∏
                # –∞–ª–µ –∫—Ä–∞—â–µ –≤—ñ–¥—Å—ñ—è—Ç–∏, —â–æ–± –Ω–µ —Ç—è–≥–Ω—É—Ç–∏ –º–µ–Ω—é
                continue

            key = (title, url)
            if key in seen:
                continue
            seen.add(key)

            desc = _pick_description(a, node if node.name != "a" else a.parent)
            items.append({"title": title, "url": url, "desc": desc})

    return items


async def _fetch_html(session: aiohttp.ClientSession, url: str) -> str:
    for _ in range(2):
        try:
            async with session.get(url, headers=DEFAULT_HEADERS, timeout=20) as r:
                if r.status == 200:
                    return await r.text()
                # —ñ–Ω–∫–æ–ª–∏ 401/403 ‚Äî —Å–ø—Ä–æ–±—É—î–º–æ –≤–¥—Ä—É–≥–µ
        except asyncio.TimeoutError:
            pass
        except Exception:
            pass
    return ""


# ---- –ü–∞—Ä—Å–µ—Ä–∏ –ø—ñ–¥ –∫–æ–∂–Ω–µ –¥–∂–µ—Ä–µ–ª–æ ----

def _sel_bbc() -> List[Tuple[str, str]]:
    # –ë–µ—Ä–µ–º–æ —Ç—ñ–ª—å–∫–∏ –±–ª–æ–∫–∏ –∑ –Ω–æ–≤–∏–Ω–∞–º–∏ (—Ç–µ, —â–æ –º—ñ—Å—Ç–∏—Ç—å /news/ –∞–±–æ /articles/)
    return [
        ('a[href*="/news/"]', "BBC"),
        ('a[href*="/articles/"]', "BBC"),
        ('article a[href*="/business/"]', "BBC"),
    ]

def _sel_reuters() -> List[Tuple[str, str]]:
    # MediaStoryCard / headings; —Å—É–≤–æ—Ä–æ —Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ –ø–æ /business|/markets|/technology
    return [
        ('article a[href*="/business/"]', "Reuters"),
        ('article a[href*="/markets/"]', "Reuters"),
        ('article a[href*="/technology/"]', "Reuters"),
        ('.media-story-card a[href*="/business/"]', "Reuters"),
        ('.media-story-card a[href*="/markets/"]', "Reuters"),
        ('.media-story-card a[href*="/technology/"]', "Reuters"),
    ]

def _sel_ft() -> List[Tuple[str, str]]:
    # FT: —Ç—ñ–∑–µ—Ä–∏ –∑ data-trackable=heading-link, —Ç–∞ —ñ–Ω—à—ñ –≤–∞—Ä—ñ–∞–Ω—Ç–∏
    return [
        ('a[data-trackable="heading-link"]', "FT"),
        ('.o-teaser__heading a', "FT"),
        ('.o-teaser a', "FT"),
    ]

def _sel_epravda() -> List[Tuple[str, str]]:
    # EP: –±–ª–æ–∫–∏ —Å—Ç–∞—Ç–µ–π –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö /finances —ñ /columns
    return [
        ('article a[href*="/finances/"]', "EP"),
        ('article a[href*="/biznes/"]', "EP"),
        ('article a[href*="/power/"]', "EP"),
        ('article a[href*="/tehnologiji/"]', "EP"),
        ('.article a[href*="/finances/"]', "EP"),
        ('.article a[href*="/columns/"]', "EP"),
        ('a[href*="/finances/"]', "EP"),
        ('a[href*="/columns/"]', "EP"),
    ]


async def parse_one(session: aiohttp.ClientSession, name: str, url: str) -> Tuple[str, List[Dict]]:
    html = await _fetch_html(session, url)
    if not html:
        return name, []

    soup = BeautifulSoup(html, "html.parser")

    # –≤–∏–±—ñ—Ä —Å–µ–ª–µ–∫—Ç–æ—Ä—ñ–≤ –ø—ñ–¥ –¥–∂–µ—Ä–µ–ª–æ
    if "BBC" in name:
        selectors = _sel_bbc()
    elif "Reuters" in name:
        selectors = _sel_reuters()
    elif "FT" in name:
        selectors = _sel_ft()
    elif "Epravda" in name:
        selectors = _sel_epravda()
    else:
        selectors = [('article a', name), ('h2 a', name), ('h3 a', name)]

    items = _collect_by_selectors(soup, selectors)

    # –¥–æ–¥–∞—Ç–∫–æ–≤–∏–π —Ñ—ñ–ª—å—Ç—Ä (–±–µ–∑ –º–µ–Ω—é/—Ñ—É—Ç–µ—Ä–∞)
    cleaned = []
    seen = set()
    for it in items:
        title = it["title"]
        url_i = it["url"]
        if _is_bad_url(url_i):
            continue
        if len(title) < TITLE_MIN_LEN:
            continue
        if not _looks_like_news_url(url_i):
            continue

        key = (title, url_i)
        if key in seen:
            continue
        seen.add(key)
        cleaned.append(it)

        if len(cleaned) >= MAX_PER_SOURCE:
            break

    return name, cleaned


async def fetch_live_grouped() -> List[Tuple[str, List[Dict]]]:
    """–ü–æ–≤–µ—Ä—Ç–∞—î [(–Ω–∞–∑–≤–∞_–¥–∂–µ—Ä–µ–ª–∞, [{title,url,desc}, ...]), ...]"""
    async with aiohttp.ClientSession() as session:
        tasks = [parse_one(session, name, url) for name, url in SOURCES.items()]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    grouped: List[Tuple[str, List[Dict]]] = []
    for i, res in enumerate(results):
        name = list(SOURCES.keys())[i]
        if isinstance(res, Exception):
            grouped.append((name, []))
        else:
            grouped.append(res)
    return grouped


def format_grouped_to_chunks(grouped: List[Tuple[str, List[Dict]]], max_chunk_len: int = 3500) -> List[str]:
    """–§–æ—Ä–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω—å ‚â§ max_chunk_len, –∑–≥—Ä—É–ø–æ–≤–∞–Ω–æ –ø–æ –¥–∂–µ—Ä–µ–ª–∞—Ö.
       –§–æ—Ä–º–∞—Ç —Ä—è–¥–∫–∞: ‚Ä¢ –ó–∞–≥–æ–ª–æ–≤–æ–∫ ‚Äî –æ–ø–∏—Å (URL)"""
    chunks: List[str] = []
    buf = "üì∞ –ê–∫—Ç—É–∞–ª—å–Ω—ñ –Ω–æ–≤–∏–Ω–∏ (–∂–∏–≤–∏–π –ø–∞—Ä—Å–∏–Ω–≥)\n"
    for source_name, items in grouped:
        # –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–æ—Ä–æ–∂–Ω—ñ –¥–∂–µ—Ä–µ–ª–∞
        if not items:
            continue
        block_header = f"\n\n<b>{source_name}</b>\n"
        if len(buf) + len(block_header) > max_chunk_len:
            chunks.append(buf)
            buf = block_header
        else:
            buf += block_header

        for it in items:
            title = _clean_text(it["title"])
            desc = _clean_text(it.get("desc", ""))
            url = it["url"]

            line = f"‚Ä¢ {title}"
            if desc and len(desc) >= 20:
                # –∫–æ—Ä–æ—Ç–∫–∏–π –æ–ø–∏—Å
                line += f" ‚Äî {desc}"
            line += f" ({url})\n"

            if len(buf) + len(line) > max_chunk_len:
                chunks.append(buf)
                buf = line
            else:
                buf += line

    if buf.strip():
        chunks.append(buf)

    # –Ø–∫—â–æ –≤–∑–∞–≥–∞–ª—ñ –Ω—ñ—á–æ–≥–æ
    if not chunks:
        chunks = ["‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–Ω–∞–π—Ç–∏ –Ω–æ–≤–∏–Ω–∏. –°–ø—Ä–æ–±—É–π —â–µ —Ä–∞–∑ —Ç—Ä–æ—Ö–∏ –ø—ñ–∑–Ω—ñ—à–µ."]

    return chunks